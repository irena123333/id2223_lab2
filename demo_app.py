# -*- coding: utf-8 -*-
"""demo_app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q-IbleUPxKojyl_2h9JyyWFeB_lENmxY

## Building a Demo

Now that we've fine-tuned our model we can build a demo to show 
off its ASR capabilities! We'll make use of ðŸ¤— Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions.

Running the example below will generate a Gradio demo where we 
can record speech through the microphone of our computer and input it to 
our fine-tuned Whisper model to transcribe the corresponding text:
"""

!pip install gradio==3.12.0
!pip install transformers==4.25.1
!pip install torch
!pip install git+https://github.com/pytube/pytube

import gradio as gr
from transformers import pipeline
from pytube import YouTube

pipe = pipeline(model="irena/whisper-small-sv-SE")

def transcribe_video(url):  
  yt=YouTube(url).streams.filter(only_audio=True).all()
  audio=yt[0].download()
  text = pipe(audio)["text"]
  return text

def transcribe_audio(audio):
  text = pipe(audio)["text"]
  return text



audio = gr.Interface(
    fn=transcribe_audio, 
    inputs=gr.Audio(source="microphone", type="filepath"), 
    outputs="text",
    title="Whisper Small Swedish",
    description="Realtime demo for Swedish speech recognition using a fine-tuned Whisper small model.",
)



video = gr.Interface(
	fn=transcribe_video,
	inputs=gr.Textbox(label="Enter a YouTube URL:"),
	outputs="text",
	title="Whisper Small Swedish",
	description="Transcribe swedish videos from YouTube",
)




demo = gr.TabbedInterface([audio, video], ["transcribe from recording", "transcribe from youtube url"])

if __name__ == "__main__":
    demo.launch(share=True)